{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 140.64MB\n",
      "Memory usage after optimization is: 114.28MB\n",
      "Decreased by 18.7%\n",
      "Memory usage after optimization is: 83.52MB\n",
      "Decreased by 40.6%\n",
      "Memory usage after optimization is: 57.15MB\n",
      "Decreased by 59.4%\n",
      "Memory usage after optimization is: 30.78MB\n",
      "Decreased by 78.1%\n",
      "Memory usage of dataframe is 191.46MB\n",
      "Memory usage after optimization is: 155.58MB\n",
      "Decreased by 18.7%\n",
      "Memory usage after optimization is: 113.69MB\n",
      "Decreased by 40.6%\n",
      "Memory usage after optimization is: 77.79MB\n",
      "Decreased by 59.4%\n",
      "Memory usage after optimization is: 41.90MB\n",
      "Decreased by 78.1%\n",
      "Memory usage of dataframe is 223.69MB\n",
      "Memory usage after optimization is: 181.76MB\n",
      "Decreased by 18.7%\n",
      "Memory usage after optimization is: 132.83MB\n",
      "Decreased by 40.6%\n",
      "Memory usage after optimization is: 90.89MB\n",
      "Decreased by 59.4%\n",
      "Memory usage after optimization is: 48.95MB\n",
      "Decreased by 78.1%\n",
      "Memory usage of dataframe is 89.22MB\n",
      "Memory usage after optimization is: 69.71MB\n",
      "Decreased by 21.9%\n",
      "Memory usage after optimization is: 50.19MB\n",
      "Decreased by 43.7%\n",
      "Memory usage after optimization is: 33.46MB\n",
      "Decreased by 62.5%\n",
      "Memory usage after optimization is: 22.31MB\n",
      "Decreased by 75.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"Loading the dataset in normal format takes up a lot of space approx. 1.4 GB when combined, This function changes\n",
    "the datatypes of all columns to reduce memory usage.\n",
    "\n",
    "As the dataset is really big effective memory management is required to work with it so as to save time and resources.\n",
    "\n",
    "\"\"\"\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" \n",
    "    iterate through all the columns of a dataframe and \n",
    "    modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(('Memory usage of dataframe is {:.2f}' \n",
    "                     'MB').format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max <\\\n",
    "                  np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max <\\\n",
    "                   np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max <\\\n",
    "                   np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max <\\\n",
    "                   np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max <\\\n",
    "                   np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max <\\\n",
    "                   np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')  \n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(('Memory usage after optimization is: {:.2f}' \n",
    "                              'MB').format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \n",
    "                                             / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset=pd.read_csv(\"2015_sales_data.csv\") \n",
    "dataset=reduce_mem_usage(dataset)\n",
    "\n",
    "dataset1=pd.read_csv(\"2016_sales_data.csv\") \n",
    "dataset1=reduce_mem_usage(dataset1)\n",
    "\n",
    "dataset2=pd.read_csv(\"2017_sales_data.csv\") \n",
    "dataset2=reduce_mem_usage(dataset2)\n",
    "\n",
    "dataset3=pd.read_csv(\"2018_sales_data.csv\") \n",
    "\n",
    "dataset3=reduce_mem_usage(dataset3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset=dataset.append(dataset1)\n",
    "dataset=dataset.append(dataset2)\n",
    "dataset=dataset.append(dataset3)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Extracting year ,month and day from the date column and also a feature day_of_week,\n",
    "which is later used to create a feature is_weekend\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "dataset['year'] = pd.DatetimeIndex(dataset['date']).year\n",
    "dataset['month'] = pd.DatetimeIndex(dataset['date']).month\n",
    "dataset['day'] = pd.DatetimeIndex(dataset['date']).day\n",
    "import datetime as dt\n",
    "dataset['date']= pd.to_datetime(dataset['date']) \n",
    "dataset['day_of_week'] = dataset.date.dt.dayofweek\n",
    "\n",
    "\n",
    "del dataset['date']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creating a features is_weekend which is a categorical feature that tells whether its a weekend or not.\n",
    "The EDA shows that the footfall and sales increases during weekend and to capture that A feature is_weekend is created.\n",
    "\n",
    "\n",
    "Not creating categorical feature of products for If the product ID was assigned sequentially so the\n",
    "latest product added would have the largest product ID.\n",
    "\n",
    "This is just a Hypothesis as I do not know how the product IDs were assigned.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def getNumber(x):\n",
    "    if x>=5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "dataset[\"is_weekend\"]=dataset[\"day_of_week\"].apply(getNumber)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The EDA shows that the footfall increases during weekend and during certain period of time every year,\n",
    "such as christmas. Normally a Feature would be created for festivals or other such periods, but I plan on using \n",
    "a Tree based model. The advantage of a Tree based model is that it automatically calculates these things and\n",
    "create the splits for the tree automatically.\n",
    "\n",
    "This can be verified by plotting the trees using graphviz library.\n",
    "\n",
    "This is why the day,year,month,day_of_week have been extracted from the columns so the tree algorithm can split\n",
    "based on these features.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Saving the data as a csv,\n",
    "but also as a pickle file so that loading it again is faster and the dtypes of the columns are stored.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset.to_csv(\"dataset.csv\",index=False)\n",
    "dataset.to_pickle(\"dataset.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "also modifying test data for the features added.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test=pd.read_csv(\"test_data.csv\") \n",
    "test=reduce_mem_usage(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['year'] = pd.DatetimeIndex(test['date']).year\n",
    "test['month'] = pd.DatetimeIndex(test['date']).month\n",
    "test['day'] = pd.DatetimeIndex(test['date']).day\n",
    "test['date']= pd.to_datetime(test['date']) \n",
    "test['day_of_week'] = test.date.dt.dayofweek\n",
    "\n",
    "\n",
    "del test['date']\n",
    "\n",
    "\n",
    "\n",
    "test[\"is_weekend\"]=test[\"day_of_week\"].apply(getNumber)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test.to_pickle(\"test.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "While the dataset of 2015 and 2016 has been useful for getting insights like increase in footfall during certain\n",
    "specific periods of the year and also when discount has been offered.\n",
    "\n",
    "The data of 2017 is more likely to predict the sales of 2018 better than 2015 and 2016.\n",
    "\n",
    "Another way would be to create 2 models one for data>=2017 and one for data<=2017 and \n",
    "assign weights to their prediction like\n",
    "\n",
    "final_prediction=0.7*pred_2017+0.3*pred_2015_16\n",
    "\n",
    "\n",
    "but that would consume a more time and memory\n",
    "\n",
    "So after generating insights from all the data, working the recent data i.e. 2017 and above seemed best for creating\n",
    "a baseline model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset=dataset[dataset['year']>=2017]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Even after discarding the 2015 and 2016 data, the dataset was still very heavy and computation was taking a lot of time.\n",
    "It was running out of memory on some models too. One solution to this was using dask instead of pandas and dask_ml\n",
    "instead of sklearn to load and train models. It would solve the out of memory issue but would still take time.\n",
    "\n",
    "another was to implement incremental learning,but that would still consume time while training.\n",
    "\n",
    "The best solution given the time limit was to take a sample of the dataset and test the hypothesis on the sample.\n",
    "\n",
    "I took 20% of the dataset as the sample and implemented few algorithms.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset=dataset.sample(frac=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I tried various model like decision tree,Xgboost,LightGBM, sklearn's implementation of GBM\n",
    "and the RMSE error was varying from 70 to 108.\n",
    "\n",
    "Finally after training random forest I got rmse score of 38 for n_estimators=10. \n",
    "\n",
    "I did manual Hyperparameter tuning for Random forest instead of using gridsearch because as the n_estimators increase\n",
    "the training time was greatly increasing.\n",
    "\n",
    "\n",
    "At last I found that at n_estimators=100, with no max_depth, \n",
    "the RF was performing good and giving an RMSE between 13 to 16.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dataset_test=dataset['sales']\n",
    "del dataset['sales']\n",
    "dataset_train=dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset_train, dataset_test, test_size = 0.25, random_state = 0)\n",
    "\n",
    "\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "n_jobs=3 parameter builds the trees Using backend ThreadingBackend with 3 concurrent workers.\n",
    "This decreases the training time.\n",
    "\n",
    "verbose=3 prints information while training like which tree is being built, so we can calculate how much time is left.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "reg=ensemble.RandomForestRegressor(n_jobs=3,verbose=3)\n",
    "\n",
    "print('='*30)\n",
    "reg.fit(X_train,Y_train)\n",
    "name=reg.__class__.__name__\n",
    "print('='*30)\n",
    "print(name)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=reg.predict(X_test)\n",
    "rmse=np.sqrt(mean_squared_error(Y_test,predictions))\n",
    "print(\"RMSError is: {}\".format(rmse))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculating the RMSE error on the test data in order to prevent leaderboard overfitting.\n",
    "Normally calculating a CV score would be a better option but the algorithm was taking too long to train, so i \n",
    "decided to go with 25% test data.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "reading the test pickle file which i had saved earlier after feature engineering.\n",
    "Reading the sample submission file and adding the prediction, converting it into int.\n",
    "saving the Dataframe.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test=pd.read_pickle(\"test.pkl\") \n",
    "pred=reg.predict(test)\n",
    "\n",
    "\n",
    "submit=pd.read_csv(\"sample_submission.csv\")\n",
    "submit['sales']=pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c=[]\n",
    "for i in submit['sales']:\n",
    "    c.append(int(i))\n",
    "    \n",
    "submit['sales']=c\n",
    "\n",
    "submit.to_csv('rforest_100estimator_dummy_2017[0.2]data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
